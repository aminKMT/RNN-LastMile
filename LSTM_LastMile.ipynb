{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB2ZLZR/zUcuBBhMwiVy2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminKMT/RNN-LastMile/blob/main/LSTM_LastMile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9DhRZqVMHjLL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample delivery sequence data\n",
        "batch_size = 32\n",
        "sequence_length = 10\n",
        "feature_dim = 6\n",
        "num_samples = 1000\n",
        "\n",
        "# Simulated data\n",
        "X_seq = torch.randn(num_samples, sequence_length, feature_dim)\n",
        "y_eta = torch.randn(num_samples) * 2 + 5  # Delivery time around 5 hours\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_eta[999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3Fccp_LXP6I",
        "outputId": "77a7aaf3-9fe0-4526-830c-e1316c4fc29b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.7376)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & Dataloader\n",
        "dataset = TensorDataset(X_seq, y_eta)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLM47d6Hv0W",
        "outputId": "b487e762-319a-47e8-e9d6-80be163f561f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7de8e3510e90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "class ETA_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        return self.fc(hn[-1]).squeeze(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "JIX-ngLrHvqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = ETA_LSTM(input_dim=feature_dim, hidden_dim=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "RRr9r38hHvi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in loader:\n",
        "        pred = model(xb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNqwL_WNH904",
        "outputId": "56ed01d5-c5b7-4fed-b963-945cd5fb1851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 258.5162\n",
            "Epoch 2: Loss = 143.4163\n",
            "Epoch 3: Loss = 145.0192\n",
            "Epoch 4: Loss = 143.5402\n",
            "Epoch 5: Loss = 145.6227\n",
            "Epoch 6: Loss = 141.2747\n",
            "Epoch 7: Loss = 140.9509\n",
            "Epoch 8: Loss = 139.2878\n",
            "Epoch 9: Loss = 143.8311\n",
            "Epoch 10: Loss = 145.9045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM with cross validation:"
      ],
      "metadata": {
        "id": "JyX8Xoie3Q2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "# Parameters\n",
        "batch_size = 32\n",
        "sequence_length = 10\n",
        "feature_dim = 6\n",
        "num_samples = 1000\n",
        "n_splits = 5\n",
        "n_epochs = 10\n",
        "\n",
        "# Simulated data\n",
        "X_seq = torch.randn(num_samples, sequence_length, feature_dim)\n",
        "y_eta = torch.randn(num_samples) * 2 + 5  # Around 5 hours\n",
        "\n",
        "# LSTM Model\n",
        "class ETA_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        return self.fc(hn[-1]).squeeze(1)\n",
        "\n",
        "# Cross-validation\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "val_losses = []\n",
        "best_model = None\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_seq)):\n",
        "    print(f\"\\n--- Fold {fold+1} ---\")\n",
        "\n",
        "    # Prepare DataLoaders\n",
        "    X_train, X_val = X_seq[train_idx], X_seq[val_idx]\n",
        "    y_train, y_val = y_eta[train_idx], y_eta[val_idx]\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = ETA_LSTM(input_dim=feature_dim, hidden_dim=64)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    print(f\"Fold {fold+1}: Validation Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nâœ… Best fold: {np.argmin(val_losses)+1}, Validation Loss = {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEErZZ5W3Lj_",
        "outputId": "a89bfd1f-935e-4d49-9a02-4d11f2d17849"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1 ---\n",
            "Epoch 1: Train Loss = 233.1860\n",
            "Epoch 2: Train Loss = 103.6068\n",
            "Epoch 3: Train Loss = 100.3196\n",
            "Epoch 4: Train Loss = 103.0483\n",
            "Epoch 5: Train Loss = 101.0623\n",
            "Epoch 6: Train Loss = 100.7240\n",
            "Epoch 7: Train Loss = 100.9774\n",
            "Epoch 8: Train Loss = 106.6424\n",
            "Epoch 9: Train Loss = 100.6920\n",
            "Epoch 10: Train Loss = 99.7164\n",
            "Fold 1: Validation Loss = 5.0596\n",
            "\n",
            "--- Fold 2 ---\n",
            "Epoch 1: Train Loss = 249.3735\n",
            "Epoch 2: Train Loss = 112.8941\n",
            "Epoch 3: Train Loss = 107.0793\n",
            "Epoch 4: Train Loss = 111.3128\n",
            "Epoch 5: Train Loss = 106.1904\n",
            "Epoch 6: Train Loss = 107.0464\n",
            "Epoch 7: Train Loss = 111.8735\n",
            "Epoch 8: Train Loss = 108.7336\n",
            "Epoch 9: Train Loss = 105.9576\n",
            "Epoch 10: Train Loss = 109.9441\n",
            "Fold 2: Validation Loss = 4.0764\n",
            "\n",
            "--- Fold 3 ---\n",
            "Epoch 1: Train Loss = 204.8273\n",
            "Epoch 2: Train Loss = 101.7706\n",
            "Epoch 3: Train Loss = 103.5812\n",
            "Epoch 4: Train Loss = 103.5988\n",
            "Epoch 5: Train Loss = 102.7374\n",
            "Epoch 6: Train Loss = 106.2769\n",
            "Epoch 7: Train Loss = 102.5395\n",
            "Epoch 8: Train Loss = 105.5044\n",
            "Epoch 9: Train Loss = 101.9200\n",
            "Epoch 10: Train Loss = 102.0352\n",
            "Fold 3: Validation Loss = 4.3165\n",
            "\n",
            "--- Fold 4 ---\n",
            "Epoch 1: Train Loss = 224.1223\n",
            "Epoch 2: Train Loss = 100.9659\n",
            "Epoch 3: Train Loss = 100.2416\n",
            "Epoch 4: Train Loss = 102.6692\n",
            "Epoch 5: Train Loss = 99.4165\n",
            "Epoch 6: Train Loss = 99.7025\n",
            "Epoch 7: Train Loss = 100.3498\n",
            "Epoch 8: Train Loss = 101.5709\n",
            "Epoch 9: Train Loss = 99.2656\n",
            "Epoch 10: Train Loss = 101.0320\n",
            "Fold 4: Validation Loss = 4.9904\n",
            "\n",
            "--- Fold 5 ---\n",
            "Epoch 1: Train Loss = 202.3528\n",
            "Epoch 2: Train Loss = 110.1438\n",
            "Epoch 3: Train Loss = 109.2666\n",
            "Epoch 4: Train Loss = 109.5512\n",
            "Epoch 5: Train Loss = 107.2342\n",
            "Epoch 6: Train Loss = 112.8498\n",
            "Epoch 7: Train Loss = 112.1144\n",
            "Epoch 8: Train Loss = 109.1954\n",
            "Epoch 9: Train Loss = 106.6802\n",
            "Epoch 10: Train Loss = 107.1326\n",
            "Fold 5: Validation Loss = 3.4272\n",
            "\n",
            "âœ… Best fold: 5, Validation Loss = 3.4272\n"
          ]
        }
      ]
    }
  ]
}